# ğŸš€ Transformers from Scratch in PyTorch

> **Rebuilding GPTâ€™s foundation â€” no shortcuts, no HuggingFace.**

This repo implements the **Transformer architecture** (the backbone of GPT and modern LLMs) from **first principles in PyTorch**.
Itâ€™s not about speed. Itâ€™s about **understanding the math â†’ code â†’ model**.

---

## ğŸ”¥ Whatâ€™s Inside

* **Scaled Dot-Product Attention**
* **Multi-Head Attention**
* **Positional Encoding**
* **Encoder & Decoder Layers**
* **End-to-End Transformer** (+ toy training loop)

---

## âš¡ Quick Start

Clone locally:

```bash
git clone https://github.com/umer-ateeq/Coding-Transformers-using-pytorch.git
cd Coding-Transformers-using-pytorch
```

---

## ğŸ“š Learn More

ğŸ“– [Full write-up on Medium](https://medium.com/@Umer-Ateeq/61c75d3457f1)

---

ğŸ’¡ *Star â­ if you find it useful â€” feedback from researchers & practitioners is welcome!*

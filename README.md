# 🚀 Transformers from Scratch in PyTorch

> **Rebuilding GPT’s foundation — no shortcuts, no HuggingFace.**

This repo implements the **Transformer architecture** (the backbone of GPT and modern LLMs) from **first principles in PyTorch**.
It’s not about speed. It’s about **understanding the math → code → model**.

---

## 🔥 What’s Inside

* **Scaled Dot-Product Attention**
* **Multi-Head Attention**
* **Positional Encoding**
* **Encoder & Decoder Layers**
* **End-to-End Transformer** (+ toy training loop)

---

## ⚡ Quick Start

Clone locally:

```bash
git clone https://github.com/umer-ateeq/Coding-Transformers-using-pytorch.git
cd Coding-Transformers-using-pytorch
```

---

## 📚 Learn More

📖 [Full write-up on Medium](https://medium.com/@Umer-Ateeq/61c75d3457f1)

---

💡 *Star ⭐ if you find it useful — feedback from researchers & practitioners is welcome!*
